\documentclass[a4paper,10pt]{book}


\author{Per Andersson, Fredrik M{\"o}llerstrand}

\title{Zohmg---a large scale data store for aggregated time-series-based
data. [DRAFT]}


\newcommand{\chapterquote}[2] {
\begin{quote}
\textit{"{#1}"}

--- {#2}
\end{quote}

\vspace{24pt}
}




\begin{document}

\maketitle

\noindent \Large{\textbf{Abstract}}

\vspace{12pt}

\noindent 
This thesis describes a data store for multi-dimensional time-series-based data. The data is aggregated over many units of measurements and across multiple dimensions. One of these dimensions is always time.

Zohmg solves the problem of summing, storing and serving measurements. These measurements may be taken from log files or similar sources. Zohmg stores only the summarized data, not the atomic measurements. Zohmg is optimized for speed of data retrieval.

The thesis is presented in three major sections: problem statement, theoretical solution and implementation details.


\tableofcontents

\vfill

\pagebreak

\chapter{Introduction}

% TODO: den har texten passar battre i ett eget stycke/sektion, background reading style.

\chapterquote{Data warehouse is a repository of an organization's
electronically stored data. Data warehouses are designed to facilitate
reporting and analysis.}{Wikipedia}

The main actions of a data warehouse is extract, transform and load data,
although not necessarily in that order.

The classic thinking in data warehousing is the divide between facts and dimensons. The facts are measures categorized by dimensions. In OLAP, a sort of data warehouse, the OLAP cube is typically modelled in fact tables and dimension tables in a relational database. The novelty of our approach is to model OLAP cube-like aggregation data store on top of a bigtable-like data store.


As large scale web services grow they generate vast volumes of log data,
for instance Apache web logs. The ability to accurately analyse gathered
data is important to the evaluation, planning, and success of the web
service. Storing and analysing massive volumes of log data requires
special measures to be taken. From an end-user perspective the requirement
is an easy, complete and fast data analysing procedure. The technical
aspects of this requirement boils down to the following key points:
Visualising, storing and importing the data. The result is an
multidimensional information system which is subject to
OLAP\footnote{On-Line Analytical Processing}. \cite{olap_solutions}

All three requirements are depend on one another, what data and how it is
stored affects what can be visualised. Also, what data is imported---and
how it is analysed---affects how it can be stored and hence visualised.



\section{Problem description}

\subsection{Scope}

Time-series based analysis is the most important one when analysing web
logs, it comes natural since all log entries are timestamped.
\cite{discoveringweb} It is also the most common form we have seen for logs
in general.

Assuming that time-series analysis is the most important, we have imposed
the restriction on the system that is built so that it only handles
time-series. Because of this restriction the data store obviously becomes
less general purpose. Although since most log data is time-series based
this restriction does not cripple the system much---if at all.



\subsection{Data size}

The sheer size of the data makes it unfeasible to use a single computer.
Eventually a single machine would be fed so much data that it cannot cope
with it. For instance a single web log file for one day can be up to
several gigabytes. It is problematic for a single machine to handle this
because of the limitations of memory, loading the entire file in memory.
Distributed computing solves this by spreading the computations out to
several machines in parallel. The obvious gain is that the waiting time
before a computation is done is divided by the amount of nodes used,
compared to running the computation on one machine. There is also a gain
related to the actual machines, it is possible to use commodity hardware.
Because MapReduce and BigTable has high availability and reliability a node
in the cluster can safely be replaced on the fly if it fails.

Scalability is also very important. Using single machines for computing is
problematic when the data set grows, more powerful machines has to be
obtained. More power contained in a single machine results in higher price,
eventually it is too expensive to perform computations on very large
data sets. Both MapReduce and BigTable scale close to linearly, adding one
machine to the cluster adds equal amount of processing power. Since
MapReduce and BigTable can use commodity hardware it is reasonable to buy
extra machines to gain extra computing power or storage space.


\section{Overview}

This thesis describes the Zohmg system and how to use it.

Chapter 2 establish the theoretical base of the problem statement.

In chapter 3, the tools used for the implementation are declared, moving
over to chapter 4 where the details of the implementation are presented.

How to use the Zohmg system is presented in chapter 5, while chapter 6
shows an example work flow.

Chapters 7 and 8 discuss the results, related, and future work, and
presents the conclusion, respectively.




\chapter{Theory}

\chapterquote{Einstein explained his theory to me every day, and on my
arrival I was fully convinced that he understood it.}{Chaim Weizmann}


\section{Data cubes}

A data cube is a conceptual n-dimensional array of values, in which the values
are numerical facts belonging to some dimension. The data cube is also known as
a hypercube, relating to multidimensional analogues of squares and cubes. In
this context, each dimension of the cube corresponds to an attribute of the
data in question and each point along the dimension's axis is a value of the
corresponding attribute. The mental model of a data cube aids in reasoning
about dimensions. Data cubes are traditionally modeled as star schemas in
relational database systems. \cite{olap_solutions}


\subsection{Dimensions}

The school book example of a data cube is three dimensional where each of
the three dimensions correspond to one of the attributes time, products,
and stores. Adding another attribute, for instance sales, adds another
dimension to the data cube, because of the one to one relationship of
attributes and dimensions.


\subsection{Projections}

One of the most common operations on a data cube is the projection. A projection is a dimensionality reduction, where one or more dimensions are discarded from the cube with the measurements along each axis summed up, leaving a data cube of a lesser dimension.

A typical projection is that from an n-dimensional cube down to two
dimensions, which is the case when plotting an object---with three or more
dimensions---on screen or paper.


\subsection{Slice and dice}

The slice operation fixes the values of one or several dimensions in the cube. The data omitted from this operation would be any data associated with the values of the dimension that were not fixed. The dice operation is a slice on more than two dimensions. [source: http://www.cs.ualberta.ca/~zaiane/courses/cmput690/glossary.html]



\section{Time series}

Time is the most common of the dimensions in the data cube constructed for Zohmg. The time stamp is a common property for all data handled by Zohmg; every measurement is expected to have time as one of its dimensions. TODO: really, this belongs in the implementation section. I find nothing interesting to say about time series in this section.



\section{Aggregates}

An aggregate is a composite of several values. Typical examples of
aggregate functions are average, count, and sum.

In a standard OLAP setup, a number of aggregates are pre-computed and
stored in the cube. These base aggregates represent only a fraction of the
many possible aggregations. The remaining aggregates are computed from the
base aggregates as they are demanded.

The main reason for pre-computing aggregates is to reduce access times, the
time it takes to compute an aggregate may be unacceptable to a waiting
user. Pre-computing frequently requested aggregates is therefore regularly
done in the background, giving the user fast access to these.
\cite{olap_solutions}

The classic trade-off between time and space applys to wether or not
pre-compute an aggregate. Pre-computing to many aggregates might render an
unfeasible of data amount to store, while pre-computing to few aggregates
renders longer access times for aggregates which are not pre-computed. This
problem is further discussed in the thesis, but left to the user of the
Zohmg system to solve.



\chapter{Tools}

\chapterquote{Arrange whatever pieces come your way.}{Virgina Wolf}

%\chapterquote{In short, intelligence, considered in what seems to be its
%original feature, is the faculty of manufacturing artificial objects,
%especially tools to make tools, and of indefinitely urging the
%manufacture.}{Henri Bergson}

Regarding the infrastructure, both BigTable and MapReduce has high
availability, scalability and reliability. Since BigTable runs on top of
GFS\footnote{Google File System}, the data stored in BigTable is
replicated, usually three times, onto other nodes in the cluster.
\cite{gfs} Likewise, if a node in a MapReduce job goes down or fails to
complete the job it is reassigned to another node. \cite{mapreduce}


% TODO> I argue that we should have 'hadoop' and 'hbase' as the main sections,
% and let mapreduce be a subsection of hadooop, bigtable a dito of hbase.

\section{MapReduce}

Analysing the massive amounts of log data raises high demand for computing
power. The MapReduce \cite{mapreduce} pattern suits the analysing issue
well. The great benefit of MapReduce is that it runs on a computer cluster
of commodity hardware where computation takes place independently in
parallel. All results computed by each node are then reduced into one
single result.

Basically MapReduce is a two phase computation. First the input data is
partitioned and each of these partitions is sent to a \textit{mapper} which
performs computations on each row of the data. The mapper then outputs each
resulting computation on the format \texttt{key,value}. These key-value
pairs are received by a \textit{reducer} which performs the second stage of
the computation.


\section{BigTable}

The data model is a simple one: Rows of data are stored in named tables.
Each row consists of a sortable row key and an arbitrary number of columns.
The columns are of the form "column-family:qualifier", where the column
family is one of a number of fixed column families defined by the table,
and the qualifier is an arbitrary string specified by the application. In
effect, column families are pre-announced while qualifiers are not. The
contents of each column family are stored together, so the user will want
to store items that have similar characteristics in the same column family.
Each data point is called a cell. A cell is an uninterpreted array of
bytes---there are no data types, and it is up to the application to
interpret the data correctly. The table can store any number of timestamped
versions of each cell, allowing versioning of data.

The model above can indeed be thought of as a sparse distributed
multidimensional sorted map. In concept, this multidimensional map is
identical to a nested hash map from your favourite programming language. The
dimensions of this multidimensional map is mapped out on the row key, the
column family and the column qualifier, and possibly also of the version
timestamp.

In actual practice, the main difference between the two models is that
querying is highly restricted in a BigTable-like data store---there are no
joins, for example---whereas the relational database was born to be queried.
On the other hand, the restrictions placed upon the BigTable models are
what allows it to scale to such a large data set. Indeed, BigTable was born
to scale.

\textbf{TODO: -- elaborate on reasons to why starsql does not scale--}

Many applications have the behaviour where they write data once and read it
a lot more, log analysers for instance. This writing-once-reading-many
behaviour is precisely what BigTable is designed for. BigTable is designed
for sequential, as opposed to random, data access. A typical use case is
the scan, where a large number of rows are read in sequence and a certain
column or column-family are picked up and returned. Certainly, you will
want to store those data points that are likely to be needed simultaneously
close to each other. \cite{bigtable}


\subsection{Data model}

Sorted distributed multidimensional sparse hashtable.


\subsubsection{Rows}

Each row in a BigTable is associated with a unique key. All the data is
stored sorted by rowkey. Because of this it is important to have a thorough
analysis of how the data is going to be accessed in order to store it
efficiently.



\subsubsection{Regions}

BigTable use the concept of tablets, HBase in turn uses regions. In order
to be consistent with the rest of this paper we henceforth use the term
region.

A region is a partition of the data stored on a node in the computer
cluster. A region server (referred to as tablet server in \cite{bigtable})
holds information on which regions are located on which nodes. More
precisely the region server knows what range of rowkeys are in each region
on each node. This behaviour makes it fast to access data while keeping
reliability and scalability features.


% TODO> why is this Tool & BigTable-subsection called Dimensions? Disambiguation needed. -fredrik
\subsection{Dimensions}

\subsubsection{Column-families}

Each column-family is stored as a single map-file in HBase.


\subsubsection{Column-qualifiers}

Each column-family is further divided into one or more column-qualifiers.


\subsection{Hierarchies}

Depending on how the rowkeys are formatted different hierarchies are
created. The order in which the data is stored is important, it defines how
effective scanning data will be. In the case of having thousand rows and
requesting ten out of these, then only one por cent of the data is
interesting. If a scan would be required to visit all the thousand rows a
lot of rows are skipped. Skipping rows is expensive in that sense that they
are but the result is thrown away. The goal is to push this cost to a bare
minimum, using as many of the rows as possible.

The minimum cost of scanning data is achieved by ordering the data so that
a scan uses every visited row. In the case with thousand rows of which ten
are wanted, would mean that ten rows scan are scanned of which all are
used.


\section{Apache Hadoop}

Hadoop is a free software implementation of Google's MapReduce-framework
described in \cite{mapreduce}. Hadoop is written in Java and is part of
the Apache Software Foundation.  It is a fairly mature piece of software
and is used in production at numerous companies. \cite{hadoop}

MapReduce is a framework for running jobs in parallel across multiple
machines. As the name implies, MapReduce is made up of two parts or
phases: map and reduce. The map phase consumes one piece of input at a
time (a piece of input can be a line from a log file, for example), and
emits an intermediate key and value. These keys and values are then
processed by the reduce phase, which emits the final key-value pairs.

MapReduce works by splitting the input, which commonly is in the size
range of many gigabytes, into n parts and lets each node of the computing
cluster work on one or more parts. The mapper on each node emits key-value
pairs which are fed into the reduce phase. The final output is collated,
usually on a distributed file system.

All Zohmg import programs are executed on Hadoop.


\subsection{Hadoop Distributed File System}

HDFS is a free software implementation of the GFS \cite{gfs}.

The goal of HDFS is to store large data sets while being able to handle
hardware failure and being deployed in heterogenous hardware and software
eco systems. Additionally the Hadoop and HDFS interaction is designed with
the notion that it is cheaper to move the computation, MapReduce programs,
to the data than the other way around.

Every Hadoop node can be formatted with HDFS, this reserves disk space on
the node to be used for HDFS. The HDFS container resides on the nodes
general purpose file system. It is also possible to use HDFS as a
stand-alone general purpose distributed file system, without Hadoop.

The default block size on HDFS 64 MB, significantly larger than
file systems used for hard drives. The motivation is that applications
that use HDFS are not general purpose applications which run on general
purpose file systems, but batch applications which read, write, or both,
large amounts of data.


\subsection{Hadoop streaming}

Any program that reads and outputs to the file pointers \texttt{stdin} and
\texttt{stdout}, respectively, can be used as MapReduce programs with
Hadoop Streaming. This makes it possible to use any shell script or
program which inputs and outputs this way as a MapReduce program.


\section{Apache HBase}

HBase is a free software implementation of Google's BigTable data store
described in \cite{bigtable}. It is a sub-project of Hadoop, written in
Java and a part of the Apache Software Foundation. HBase is still in its
infancy and few organizations use it for mission-critical applications.
\cite{hbase}

BigTable is a data store designed for traversal of very large data sets.
Time-stamped cells inside column-familes in rows. The row has a key and
any number of cells attached to it. The rows in a table are sorted by the
row key.  The typical use-case of BigTable is to scan a range of rows. It
is therefor important to set up one's keys so that related data is close
to each other. For example, if it makes sense to traverse the data in
chronological order, the key might contain a representation of the data's
timestamp. The classic example is to have the reversed domain name (i.e.
com.google.www) as the row key, which means that all sub-domains of any
domain are next to each other.

Sparseness is a big thing. Null values are free of charge. TODO: *Describe
how we leverage sparsity*.

TODO: *this should go into implementation.*
The typical use case for Zohmg is to read one or more column-qualifiers,
which correspond to one or more points in n-space, over a range of time.
Therefor, the keys are made up of the time and unit of the measurement,
and the qualifier queried for represents the point in n-space. More on
this in the data model section.

All Zohmg data is stored in HBase.


\subsection{Thrift}

A remote procedure call framework for building scalable cross-language
services. Thrift combines a software stack with a code generation engine to
build services that work seamlessly between a wide range of programming and
scripting languages. \cite{thrift}

Thrift was originally developed at Facebook and in April 2007 it was
released as free software. It entered the Apache Incubator in May 2008.

HBase has a Thrift server which serves as the interface to languages other
than Java--for instance Python.



\section{Dumbo}

Dumbo is a framework for writing Hadoop programs in languages other than
Java, in a so-called Streaming Mode. Dumbo is written in Python. It is used
extensively at Last.fm for writing short prototypes. \cite{dumbo}

All Zohmg programs written in Python use Dumbo to execute on Hadoop.


\section{Serializing}

Serializing data is the task of converting an object into a sequence of
bits so that it can be stored on a storage medium or transmitted over a
network link. Rereading the sequence of bits should restore the original
state of the object it was in before the serialization.


\subsection{JSON}

JSON\footnote{JavaScript Object Notation} is a lightweight text-based
human-readable data interchange format. It is used for representing simple
data structures and associative arrays (called objects). JSON is often used
for serializing structured data and transmitting it over network
connection.


\subsection{YAML}

YAML\footnote{YAML Ain't a Markup Language} is a human-readable data
serialization format available for a wide range of programming and
scripting languages.

The outline and markup of YAML documents makes it well suited for data
which humans are likely to view or edit, such as configuration files.



\section{Java}

Both Hadoop and HBase are written in Java---a high-level object-oriented
programming language.

Most Hadoop users write their MapReduce programs in Java, since this is the
native language of Hadoop. \cite{java}



\section{Python}

Python is a general-purpose high-level multi-paradigm---amongst others:
object-oriented, imperative and functional---scripting language.
\cite{python}

Zohmg is written mostly in Python. Zohmg uses the following Python modules
internally; it depends on them to be available on the system.


\subsection{Paste}

Python Paste is a framework for building WSGI\footnote{Web Server Gateway
Interface, a HTTP extension where a CGI-like environment is passed around.}
applications and middleware. \cite{definitive_guide_to_pylons}

WSGI applications receive a WSGI request and returns a response with the
built-in web server.

Middleware is software that acts as an intermediary. WSGI middleware
receives a WSGI request and then performs logic upon this request, before
passing the request to a WSGI application. \cite{paste}


\subsection{PyYAML}

PyYAML is a complete YAML 1.1 compliant parser and emitter for Python.
\cite{pyyaml}


\subsection{Setuptools}

Python Setuptools is a framework---built on top of the Python module
distutils---for basic package management of Python software. Amongst other
things it creates and installs Python Eggs\footnote{A Zip file which
bundles Python software files.}. \cite{setuptools}


\subsection{simplejson}

A simple, fast, complete and extensible JSON encoder and decoder for
Python. The Python module simplejson is implemented entirely in Python and
has no external dependencies. However a C extension which provides a
serious speed boost is included in simplejson. \cite{simplejson}




\chapter{Implementation}

%\chapterquote{A good idea is about ten percent and implementation and hard
%work, and luck is 90 percent.}{Guy Kawasaki}

\chapterquote{It is not always what we know or analyzed before we make a
decision that makes it a great decision. It is what we do after we make the
decision to implement and execute it that makes it a good
decision.}{William Pollard}

% COMMENT: do we really need to repeat this?
As stated in chapter 3, Zohmg is built upon existing tools and software. It
uses Dumbo to run jobs on Apache Hadoop, and uses Apache HBase to persist the
data.


% TODO: not sure this is precisely a story about the implementation. putting it here for now.

-- A truly magnetic data store - the idea of enabling the user --

From a user's perspective, storing aggregates in Zohmg is as simple as writing
a map (in the MapReduce sense) and letting Zohmg run that map over the data
set. The user's map is called \textit{usermap} from here on. In order to let Zohmg
know what output to expect from the mapper, the user also defines the
dimensions and units of his data. After all, the user most likely knows his
own data better than anyone else.

A standard MapReduce map emits key-value pairs. Not so in the case of the
Zohmg mapper: the usermap emits triples (that is, a tuple of three values)
consisting of a timestamp, a list of dimensions and their respective values,
and a list of measurements. In effect, the dimensions and the timestamp
specify a point in n-space and the measurements represent the observations
made at that point.

Going back to the running example of apache access logs, a usermap might look
something like this:

\begin{verbatim}
def map(key, value):
  log = parse_apache_log(value)
  dimensions = {'useragent' : log.useragent,
                'path'      : log.path,
                'status'    : log.statuscode}
  values = {'pageviews' : 1,
            'bytes'     : log.size}

 yield parsed.timestamp, dimensions, measurements
\end{verbatim}

\vspace{12pt}

The actual parsing of the log is abstracted; it is the emitting of
dimensionality data I want to showcase in this example. We assume that
\texttt{parse\_apache\_log()} accepts a single line from the log file and
returns an object with the appropriate fields, such as useragent ant
statuscode.

For every line of the log file, we emit a point in n-space (the dimensions and
the timestamp) together with a count of pageviews and bytes; one pageview per
request and the number of bytes served in the request. It is likely that you
will want to add business logic here, perhaps only counting requests from
useragents that aren not spiders and that resulted in a status code of 200.

The last line \textit{yields} the tuple. This is a python construct called a
generator that lets the map emit any number of tuples (or none!) for a single
invocation.

The user will import a day worth of logs like so:

\begin{verbatim}
  zohmg import mappers/apache.py /data/weblogs/2009/05/10
\end{verbatim}

This will instruct zohmg to apply the map function found in \texttt{apache.py}
to all logs that it can find in the \texttt{/data/weblogs/2009/05/10} directory
on HDFS. The emitted output is duly collected and projections are carefully
stored in the data store.




- Something about cardinality -

When reasoning about dimensions, the issue of cardinality will come up sooner
or later.

The cardinality of a dimension is the number of distinct values of the
dimension. For example, the dimension of \textit{logged in} has a cardinality
of two since \textit{logged in} is a boolean variable; the user is either
logged in or not. The \textit{country} dimension has a much higher
cardinality: there are some 300 countries in the world. Certainly, there could
be dimension whose cardinality is infinite: the \textit{path} dimension in the
web log example, for example, could in theory have a limitless number of
values.

The number of possible combinations of dimensions of your data---the number
of points in n-space---is directly related to the cardinality of each
dimension. It is likely that your data is of such a nature that it is
physically impossible to store information about all dimension combinations.
This is why we ask you, the user, to define what projections you are
interested in: storing aggregates for the slices of the cube that interest you
is both a time and space save, and in many cases the only feasible solution.

-- Limits on cardinality --

[What are the limits of our system? How high cardinality do we support? (This
is closely tied to hbase and the qualifier-limit.) If Martin is going to have
\textit{normalized\_path} as one of his dimensions, we might run into trouble.]

-- The implementation of all that --

Since we wish to store projections rather than all combinations of all
dimensions, we need to mangle the data slightly before sending it down the
pipe to the underlying data store.

Dumbo expects a key-value pair from the mapper. Hadoop will collect all keys
and give the reducer a key and an iterator of values. If we are to store
projections, we need to encode both the desired dimensions and the unit in
question in the key and emit the value of the unit in the \textit{value} field.
This way we can use a simple SumReducer to sum the values for each unit at any
point in n-space and be done.

Remember, the usermapper gives us a point in n-space and a list of
measurements. We need to break out the measurements and sum over each such
measurement. Zohmg solves this by wrapping the usermapper in a cleverly named
devicie, namely a mapperwrapper.

% TODO: this para sounds as if it was written by a drunken monkey. The
mapperwrapper takes each tuple emitted by the usermapper and performs a
dimensionality reduction, ending up with the projections requested by the
user. For every projection and unit, the dimensions remaining after the
reduction are appended to the string representation of the unit and emitted as
the key. The value is simply the value of the unit. The Hadoop framework will
do the heavy work of collecting all keys and summing the values.

An alternative approach would have been to perform the dimensionality
reduction at a later stage, namely in the reducer of a multi-pass MapReduce
job.

The reducer, then, is very simple: it's a simple sum of the values. The output
of the reducer will not be stored on disk as is usually the case, but will
instead be interpreted by a custom OutputReader that persists the data to
HBase.

The reducer needs to communicate to the OutputReader under what rowkey,
column-family and qualifier to store the computed sum. It does this by
emitting the row key as the key and a json-serialized map of the form
{"column-family:qualifier" : value}.

The OutputReader interprets the json and persists the data. Et voila!


\section{Configuration}

\subsection{Data sets}

% TODO: define what a data set is, and what makes it different from a project.

The user's data sets are defined using YAML. The data set files reside in
the project's \texttt{config} directory.


\subsection{Environment}

Certain paths need to be known by Zohmg in order to function, these are set
in the config/environment.py file.



\section{Importing}

The user writes a mapper which Zohmg wraps, interpreting the output from
the mapper program and storing it in HBase.

Support exists for writing the mapper in Java or Python.



\section{Data interchange}

\subsection{Internal}

\subsubsection{Input formats}

\subsubsection{Output formats}

% TODO: not truly correct
The Darling output format for HBase. Between Zohmg and Hadoop JSON is used
for serializing data.


\subsection{External}

The data export server is built with Python Paste---a middleware
framework---the data server dispatches to the requested application
depending on the URI it was fed.


\subsubsection{URLParser}

Middleware which dispatches to applications based on the URI. The URLParser
removes the first part of the URI and tries to find a matching application
to relay the request to. For instance, consider the URI
\texttt{/client/file.html}. The first part---\texttt{client}---will be
removed and the request \texttt{/file.html} will be sent to the application
named \texttt{client.py}.


\subsubsection{Clients}

Dispatches request to serve static files from the project's
\texttt{clients} directory. Custom clients---which interact with Zohmg---can
be built with HTML and JavaScript, see the examples.


\subsubsection{Data}

% ?
Fetches data from HBase via its Thrift interface. The data is served to the
user as JSONP\footnote{JSON with padding: JSON extension which adds name of
callback function as an argument to the function itself.}.


\subsubsection{Transformers}

Fetches data from HBase and transforms it with the requested transform
program from the project's transformers directory, output is
dumped as JSONP.





\chapter{Usage}

\chapterquote{There comes a time when all people break down and do the
unthinkable: They read the manual.}{Unknown}

This is the user manual.

\section{Deployment}

A manage script is used to create tables from a pre-defined schema.

\begin{verbatim}
   zohmg create project
\end{verbatim}

\noindent Edit the configuration files, which are text files in YAML and
Python format.

\begin{verbatim}
   zohmg setup
\end{verbatim}

\noindent The above command executes the configuration made and creates the
infrastructure (HBase tables) for the project.


\section{Import}

The user writes the map part of the MapReduce programs which imports
log-like data to the data store. The reduce step is taken care of by Zohmg,
which uses sum reducer that simply sums up values with the same keys.


\subsection{User mappers}

Import jobs use mappers written by the user, either in Python or Java.
Mappers written by the user import each line from the input file and set
the arguments to the map function, key and value, as line number and line
contents, respectively. The user can then perform computations on these
entities and finally yield the results as a dictionary. The resulting
dictionary will be interpreted by Zohmg and stored accordingly.

\begin{verbatim}
   zohmg import mappers/somemapper.py hdfs/path
\end{verbatim}


\section{Storing data}

% COMMENT: I don't think we should mention the underlying store in the usage section.
Data is stored in Apache HBase. A natural consequence of this that the
data is stored sorted by rowkey allowing fast access to an arbitrary
(sequential) range.



\section{Export}

It is possible to query Zohmg about metadata it has on each of the imported
data sets.

Data is served to the user either over HTTP in JSON format or over Thrift.

\begin{verbatim}
   zohmg serve
\end{verbatim}

\noindent The above serves on default port 8086, with the \texttt{--port}
argument it is possible to change server port.

The data server have three methods of serving data: raw, transformed, or
static. The raw and transformed data are aggregates served from HBase,
transformed in the latter case, while the static method just serve static
files from the project's \texttt{clients} directory. Aggregates served from
HBase are returned to the user as JSONP.


\subsection{Data}

Based on the query---which the user submits---the corresponding data is
served from HBase as JSONP. The query is a simple definition of wanted row
range, dimensions, and values for these dimensions. The query is submitted
to the data server via HTTP, an example query could look like (the URL is
wrapped to fit the page) \\

\texttt{http://localhost:8086/data/?t0=20090101 \\
\&t1=20090420\&unit=pageviews\&d0=country\&d0v=SE} \\

The above query would return the pageviews for SE between 1 January 2009
and 4 April 2009.


\subsection{Transformers}

Fetches data from HBase and transforms it with the requested transform
program from the project's \texttt{transformers} directory, output is
dumped as JSONP.


\subsection{Clients}

It is possible to build static web pages out of HTML and JavaScript, which
can interact with the data server. These static pages can for instance
present some sort of query building mechanism---for instance with drop down
boxes---and then send this query to the data server and graph the resulting
data.

The data server serves static files from the project's \texttt{clients}
directory at the server URL appended with \texttt{client/filename}.



\chapter{Examples}

\chapterquote{Example is not the main thing in influencing others, it's
the only thing}{Albert Schweitzer}

%\chapterquote{There is nothing so annoying as a good example!!}{Mark Twain}

%\chapterquote{Example isn't another way to teach, it is the only way to
%teach}{Albert Einstein}


\section{Web log analysis}

Configure the dataset.yaml.

Write a custom mapper, which imports data from web logs.

Start data server and extract data from it.

Possibly write a transformer which performs some transformation on the
aggregated data.

Possibly write a client which will render a graph.



\chapter{Discussion}

\chapterquote{In discussion comes the light!}{Unknown}

\section{Related work}

HDW is system which goal is to create a high performance large scale data
warehouse based on MapReduce and Bigtable. \cite{hdw}



\section{Future work}

Says Wikipedia: "Organizations generally start off with relatively simple use of data warehousing. Over time, more sophisticated use of data warehousing evolves." Zohmg is that relatively simple data warehouse; the seed from which a more sophisticated data warehouse may evolve.

There is as of yet no support for specifying granularities. The only supported resolution of time is the day.

The single aggregation performed is summing of the measurements. Future work may include average, max and min, etc.

There is no metadata store. It would be very helpful for the application writer to know what the known values are for each dimension.

It'd be interesting to think a bit about how the data store could be fully elastic, i.e. not require any data set definition from the user and instead create column-families on the fly based on the dimensions output from the user's map.

\subsection{Multiple data sets}

\subsection{Metadata}

\subsection{Updating data set files}

Update tables on changes in configuration---data set files.

\subsection{Importing without MapReduce}

Importing content without firing of a MapReduce job, e.g. via a Thrift
service.



\chapter{Conclusion}

\chapterquote{When we can't dream any longer we die.}{Emma Goldman}

Last.fm is looking at using Zohmg for multiple application areas, such as
radio stats and web page stats.


\pagebreak



\bibliographystyle{plain} \bibliography{references}



\end{document}
