\documentclass[a4paper,10pt]{book}

\author{Per Andersson, Fredrik M{\"o}llerstrand}
\title{Zohmg---a large scale data store for aggregated time-series-based
data. [DRAFT]}



\begin{document}



\maketitle



\noindent \Large{\textbf{Abstract}}

\vspace{12pt}

\noindent This is where we put down two to four sentences about:
Introduction, problem, existing tools, our solution and implementation,
conclusions.


\tableofcontents

\vfill

\pagebreak



\chapter{Introduction}

This thesis talks about a data store for aggregated multi-dimensional time-series-based data. The data is aggregated over many units of measurements and across multiple dimensions. One of these dimensions is always time.

Zohmg solves the problem of summing and storing measurements taken from log files or similar. It stores solely summarized data, not atomic data. Zohmg is optimized for speed of data retrieval.

The thesis is presented in three major sections: problem statement, the theoretical solution and the implementation details.

Says Wikipedia: "Data warehouse is a repository of an organization's electronically stored data. Data warehouses are designed to facilitate reporting and analysis." The main actions of a data warehouse is extract, transform and load data, although not necessarily in that order.

The classic thinking in data warehousing is the divide between facts and dimensons. The facts are measures categorized by dimensions. In OLAP, a sort of data warehouse, the OLAP cube is typically modelled in fact tables and dimension tables in a relational database. The novelty of our approach is to model OLAP cube-like aggregation data store on top of a bigtable-like data store.


As large scale web services grow they generate vast volumes of log data,
for instance Apache web logs. The ability to accurately analyse gathered
data is important to the evaluation, planning, and success of the web
service. Storing and analysing massive volumes of log data requires
special measures to be taken. From an end-user perspective the requirement
is an easy, complete and fast data analysing procedure. The technical
aspects of this requirement boils down to the following key points:
Visualising, storing and importing the data. The result is an
multidimensional information system which is subject to
OLAP\footnote{On-Line Analytical Processing}. \cite{olap_solutions}

All three requirements are depend on one another, what data and how it is
stored affects what can be visualised. Also, what data is imported---and
how it is analysed---affects how it can be stored and hence visualised.

It is assumed that the audience of this paper has at least a basic
understanding of how one models data in relational databases. As will
quickly become apparent, a BigTable-like \cite{bigtable} data store is
drastically different from the relational model, meaning that those who
know their table schemas will have some un-learning to do before being able
to fully grasp the leaner BigTable-model.

% TODO: necessity of this paragraph?
The reader is also assumed to possess basic understanding about distributed
systems, with focus on high level abstractions rather than detailed
knowledge about practical implementation details.



\section{Problem description}

\subsection{Scope}

Time-series based analysis is the most important one when analysing web
logs, it comes natural since all log entries are timestamped.
\cite{discoveringweb} It is also the most common form we have seen for logs
in general.

Assuming that time-series analysis is the most important, we have imposed
the restriction on the system that is built so that it only handles
time-series. Because of this restriction the data store obviously becomes
less general purpose. Although since most log data is time-series based
this restriction does not cripple the system much---if at all.



\subsection{Data size}

The sheer size of the data makes it unfeasible to use a single computer.
Eventually a single machine would be fed so much data that it cannot cope
with it. For instance a single web log file for one day can be up to
several gigabytes. It is problematic for a single machine to handle this
because of the limitations of memory, loading the entire file in memory.
Distributed computing solves this by spreading the computations out to
several machines in parallel. The obvious gain is that the waiting time
before a computation is done is divided by the amount of nodes used,
compared to running the computation on one machine. There is also a gain
related to the actual machines, it is possible to use commodity hardware.
Because MapReduce and BigTable has high availability and reliability a node
in the cluster can safely be replaced on the fly if it fails.

Scalability is also very important. Using single machines for computing is
problematic when the data set grows, more powerful machines has to be
obtained. More power contained in a single machine results in higher price,
eventually it is too expensive to perform computations on very large
data sets. Both MapReduce and BigTable scale close to linearly, adding one
machine to the cluster adds equal amount of processing power. Since
MapReduce and BigTable can use commodity hardware it is reasonable to buy
extra machines to gain extra computing power or storage space.


\section{Overview}

This thesis fully describes the Zohmg system and how to use it.

Chapter 2 establish the theoretical base of the problem statement.

In chapter 3 the tools used for the implementation are declared, moving
over to chapter 4 where the details of the implementation are presented.

How to use the Zohmg system is presented in chapter 5, while chapter 6
shows an example work flow.

Chapters 7 and 8 discuss the results, related, and future work, and
presents the conclusion, respectively.




\chapter{Theory}

\section{Data cubes}

A data cube is a conceptual n-dimensional array of values. The values are
numerical facts belonging to some dimension. The data cube is also known as a
hypercube, it relates to multidimensional analogues of squares and cubes.
In this context, each dimension of the cube corresponds to an attribute of
the data in question and each point along the dimension's axis is a value of
the corresponding attribute. The mental model of a data cube aids in
reasoning about dimensions. Data cubes are traditionally modeled as star
schemas in relational database systems. \cite{olap_solutions}


\subsection{Dimensions}

The school book example of a data cube is three dimensional where each of
the three dimensions correspond to one of the attributes time, products,
and stores. Adding another attribute, for instance sales, adds another
dimension to the data cube, because of the one to one relationship of
attributes and dimensions.


\subsection{Projections}

One of the most common operations on a data cube is the projection. A projection is a dimensionality reduction, where one or more dimensions are discarded from the cube with the measurements along each axis summed up, leaving a data cube of a lesser dimension.

A typical projection is that from an n-dimensional cube down to two
dimensions, which is the case when plotting an object---with three or more
dimensions---on screen or paper.


\subsection{Slice and dice}

The slice operation fixes the values of one or several dimensions in the cube. The data omitted from this operation would be any data associated with the values of the dimension that were not fixed. The dice operation is a slice on more than two dimensions. [source: http://www.cs.ualberta.ca/~zaiane/courses/cmput690/glossary.html]



\section{Time series}

Time is the most common of the dimensions in the data cube constructed for Zohmg. The time stamp is a common property for all data handled by Zohmg; every measurement is expected to have time as one of its dimensions. TODO: really, this belongs in the implementation section. I find nothing interesting to say about time series in this section.



\section{Aggregates}

An aggregate is a composite of several values. Typical examples of
aggregate functions are average, count, and sum.

In a standard OLAP setup, a number of aggregates are pre-computed and
stored in the cube. These base aggregates represent only a fraction of the
many possible aggregations. The remaining aggregates are computed from the
base aggregates as they are demanded.

The main reason for pre-computing aggregates is to reduce access times, the
time it takes to compute an aggregate may be unacceptable to a waiting
user. Pre-computing frequently requested aggregates is therefore regularly
done in the background, giving the user fast access to these.
\cite{olap_solutions}

The classic trade-off between time and space applys to wether or not
pre-compute an aggregate. Pre-computing to many aggregates might render an
unfeasible of data amount to store, while pre-computing to few aggregates
renders longer access times for aggregates which are not pre-computed. This
problem is further discussed in the thesis, but left to the user of the
Zohmg system to solve.



\chapter{Tools}

Regarding the infrastructure, both BigTable and MapReduce has high
availability, scalability and reliability. Since BigTable runs on top of
GFS\footnote{Google File System}, the data stored in BigTable is
replicated, usually three times, onto other nodes in the cluster.
\cite{gfs} Likewise, if a node in a MapReduce job goes down or fails to
complete the job it is reassigned to another node. \cite{mapreduce}


\section{MapReduce}

Analysing the massive amounts of log data raises high demand for computing
power. The MapReduce \cite{mapreduce} pattern suits the analysing issue
well. The great benefit of MapReduce is that it runs on a computer cluster
of commodity hardware where computation takes place independently in
parallel. All results computed by each node are then reduced into one
single result.

% ONE DOES NOT SIMPLY PROCESS THE DATA ON ONE COMPUTER INTO MORDOR.
Basically MapReduce is a two phase computation. First the input data is
partitioned and each of these partitions is sent to a \textit{mapper} which
performs computations on each row of the data. The mapper then outputs each
resulting computation on the format \texttt{key,value}. These key-value
pairs are received by a \textit{reducer} which performs the second stage of
the computation.


\section{BigTable}

The data model is a simple one: Rows of data are stored in named tables.
Each row consists of a sortable row key and an arbitrary number of columns.
The columns are of the form "column-family:qualifier", where the column
family is one of a number of fixed column families defined by the table,
and the qualifier is an arbitrary string specified by the application. In
effect, column families are pre-announced while qualifiers are not. The
contents of each column family are stored together, so the user will want
to store items that have similar characteristics in the same column family.
Each data point is called a cell. A cell is an uninterpreted array of
bytes---there are no data types, and it is up to the application to
interpret the data correctly. The table can store any number of timestamped
versions of each cell, allowing versioning of data.

The model above can indeed be thought of as a sparse distributed
multidimensional sorted map. In concept, this multidimensional map is
identical to a nested hash map from your favourite programming language. The
dimensions of this multidimensional map is mapped out on the row key, the
column family and the column qualifier, and possibly also of the version
timestamp.

In actual practice, the main difference between the two models is that
querying is highly restricted in a BigTable-like data store---there are no
joins, for example---whereas the relational database was born to be queried.
On the other hand, the restrictions placed upon the BigTable models are
what allows it to scale to such a large data set. Indeed, BigTable was born
to scale.

\textbf{TODO: -- elaborate on reasons to why starsql does not scale--}

Many applications have the behaviour where they write data once and read it
a lot more, log analysers for instance. This writing-once-reading-many
behaviour is precisely what BigTable is designed for. BigTable is designed
for sequential, as opposed to random, data access. A typical use case is
the scan, where a large number of rows are read in sequence and a certain
column or column-family are picked up and returned. Certainly, you will
want to store those data points that are likely to be needed simultaneously
close to each other. \cite{bigtable}


\subsection{Data model}

Sorted distributed multidimensional sparse hashtable.


\subsubsection{Rows}

Each row in a BigTable is associated with a unique key. All the data is
stored sorted by rowkey. Because of this it is important to have a thorough
analysis of how the data is going to be accessed in order to store it
efficiently.



\subsubsection{Regions}

BigTable use the concept of tablets, HBase in turn uses regions. In order
to be consistent with the rest of this paper we henceforth use the term
region.

A region is a partition of the data stored on a node in the computer
cluster. A region server (referred to as tablet server in \cite{bigtable})
holds information on which regions are located on which nodes. More
precisely the region server knows what range of rowkeys are in each region
on each node. This behaviour makes it fast to access data while keeping
reliability and scalability features.



\subsection{Dimensions}

\subsubsection{Column-families}

Each column-family is stored as a single map-file in HBase.


\subsubsection{Column-qualifiers}

Each column-family is further divided into one or more column-qualifiers.


\subsection{Hierarchies}

Depending on how the rowkeys are formatted different hierarchies are
created. The order in which the data is stored is important, it defines how
effective scanning data will be. In the case of having thousand rows and
requesting ten out of these, then only one por cent of the data is
interesting. If a scan would be required to visit all the thousand rows a
lot of rows are skipped. Skipping rows is expensive in that sense that they
are but the result is thrown away. The goal is to push this cost to a bare
minimum, using as many of the rows as possible.

The minimum cost of scanning data is achieved by ordering the data so that
a scan uses every visited row. In the case with thousand rows of which ten
are wanted, would mean that ten rows scan are scanned of which all are
used.


\section{Apache Hadoop}

Hadoop is a free software implementation of Google's MapReduce-framework
described in \cite{mapreduce}. Hadoop is written in Java and is part of
the Apache Software Foundation.  It is a fairly mature piece of software
and is used in production at numerous companies. \cite{hadoop}

MapReduce is a framework for running jobs in parallel across multiple
machines. As the name implies, MapReduce is made up of two parts or
phases: map and reduce. The map phase consumes one piece of input at a
time (a piece of input can be a line from a log file, for example), and
emits an intermediate key and value. These keys and values are then
processed by the reduce phase, which emits the final key-value pairs.

MapReduce works by splitting the input, which commonly is in the size
range of many gigabytes, into n parts and lets each node of the computing
cluster work on one or more parts. The mapper on each node emits key-value
pairs which are fed into the reduce phase. The final output is collated,
usually on a distributed file system.

All Zohmg import programs are executed on Hadoop.


\subsection{Hadoop Distributed File System}

HDFS is a free software implementation of the GFS \cite{gfs}.

The goal of HDFS is to store large data sets while being able to handle
hardware failure and being deployed in heterogenous hardware and software
eco systems. Additionally the Hadoop and HDFS interaction is designed with
the notion that it is cheaper to move the computation, MapReduce programs,
to the data than the other way around.

Every Hadoop node can be formatted with HDFS, this reserves disk space on
the node to be used for HDFS. The HDFS container resides on the nodes
general purpose file system. It is also possible to use HDFS as a
stand-alone general purpose distributed file system, without Hadoop.

The default block size on HDFS 64 MB, significantly larger than
file systems used for hard drives. The motivation is that applications
that use HDFS are not general purpose applications which run on general
purpose file systems, but batch applications which read, write, or both,
large amounts of data.


\subsection{Hadoop streaming}

Any program that reads and outputs to the file pointers \texttt{stdin} and
\texttt{stdout}, respectively, can be used as MapReduce programs with
Hadoop Streaming. This makes it possible to use any shell script or
program which inputs and outputs this way as a MapReduce program.


\section{Apache HBase}

HBase is a free software implementation of Google's BigTable data store
described in \cite{bigtable}. It is a sub-project of Hadoop, written in
Java and a part of the Apache Software Foundation. HBase is still in its
infancy and few organizations use it for mission-critical applications.
\cite{hbase}

BigTable is a data store designed for traversal of very large data sets.
Time-stamped cells inside column-familes in rows. The row has a key and
any number of cells attached to it. The rows in a table are sorted by the
row key.  The typical use-case of BigTable is to scan a range of rows. It
is therefor important to set up one's keys so that related data is close
to each other. For example, if it makes sense to traverse the data in
chronological order, the key might contain a representation of the data's
timestamp. The classic example is to have the reversed domain name (i.e.
com.google.www) as the row key, which means that all sub-domains of any
domain are next to each other.

Sparseness is a big thing. Null values are free of charge. TODO: *Describe
how we leverage sparsity*.

TODO: *this should go into implementation.*
The typical use case for Zohmg is to read one or more column-qualifiers,
which correspond to one or more points in n-space, over a range of time.
Therefor, the keys are made up of the time and unit of the measurement,
and the qualifier queried for represents the point in n-space. More on
this in the data model section.

All Zohmg data is stored in HBase.


\subsection{Thrift}

A remote procedure call framework for building scalable cross-language
services. Thrift combines a software stack with a code generation engine to
build services that work seamlessly between a wide range of programming and
scripting languages. \cite{thrift}

Thrift was originally developed at Facebook and in April 2007 it was
released as free software. It entered the Apache Incubator in May 2008.

HBase has a Thrift server which serves as the interface to languages other
than Java--for instance Python.



\section{Dumbo}

Dumbo is a framework for writing Hadoop programs in languages other than
Java, in a so-called Streaming Mode. Dumbo is written in Python. It is used
extensively at Last.fm for writing short prototypes. \cite{dumbo}

All Zohmg programs written in Python use Dumbo to execute on Hadoop.



\section{Serializing}

Serializing data is the task of converting an object into a sequence of
bits so that it can be stored on a storage medium or transmitted over a
network link. Rereading the sequence of bits should restore the original
state of the object it was in before the serialization.


\subsection{JSON}

JSON\footnote{JavaScript Object Notation} is a lightweight text-based
human-readable data interchange format. It is used for representing simple
data structures and associative arrays (called objects). JSON is often used
for serializing structured data and transmitting it over network
connection.


\subsection{YAML}

YAML\footnote{YAML Ain't a Markup Language} is a human-readable data
serialization format available for a wide range of programming and
scripting languages.

The outline and markup of YAML documents makes it well suited for data
which humans are likely to view or edit, such as configuration files.



\section{Java}

Both Hadoop and HBase are written in Java---a high-level object-oriented
programming language.

Most Hadoop users write their MapReduce programs in Java, since this is the
native language of Hadoop. \cite{java}



\section{Python}

Python is a general-purpose high-level multi-paradigm---amongst others:
object-oriented, imperative and functional---scripting language.
\cite{python}

Zohmg is written mostly in Python. Zohmg uses the following Python modules
internally; it depends on them to be available on the system.


\subsection{Paste}

Python Paste is a framework for building WSGI\footnote{Web Server Gateway
Interface, a HTTP extension where a CGI-like environment is passed around.}
applications and middleware. \cite{definitive_guide_to_pylons}

WSGI applications receive a WSGI request and returns a response with the
built-in web server.

Middleware is software that acts as an intermediary. WSGI middleware
receives a WSGI request and then performs logic upon this request, before
passing the request to a WSGI application. \cite{paste}


\subsection{PyYAML}

PyYAML is a complete YAML 1.1 compliant parser and emitter for Python.
\cite{pyyaml}


\subsection{Setuptools}

Python Setuptools is a framework---built on top of the Python module
distutils---for basic package management of Python software. Amongst other
things it creates and installs Python Eggs\footnote{A Zip file which
bundles Python software files.}. \cite{setuptools}


\subsection{simplejson}

A simple, fast, complete and extensible JSON encoder and decoder for
Python. The Python module simplejson is implemented entirely in Python and
has no external dependencies. However a C extension which provides a
serious speed boost is included in simplejson. \cite{simplejson}



\chapter{Implementation}

As stated in chapter 3, Zohmg is built upon existing tools and software.
It uses Apache Hadoop and Apache HBase, free software implementations of
Google's MapReduce and BigTable respectively, both implemented in Java.

Importing data to Zohmg is as simple as the user defines a mapper---the
first half of a MapReduce program---which is run over a file that resides
on HDFS\footnote{Hadoop Distributed Filesystem}.

The resulting aggregated data, from the mapper, is stored in Apache HBase.

Exporting data from Zohmg is done via a web interface which dumps JSON.
It is possible to transform the stored data before exporting it with user
defined transformers.

Zohmg is implemented in Python.

Using Zohmg the user first creates a project directory. This is further
explained in chapter 5.


\section{Configuration}

\subsection{Data sets}

The user's data sets are defined using YAML. The data set files reside in
the project's \texttt{config} directory.


\subsection{Environment}

Certain paths need to be known by Zohmg in order to function, these are set
in the config/environment.py file.



\section{Importing}

The user writes a mapper which Zohmg wraps, interpreting the output from
the mapper program and storing it in HBase.

Support exists for writing the mapper in Java or Python.



\section{Data interchange}

\subsection{Internal}

\subsubsection{Input formats}

\subsubsection{Output formats}

The Darling output format for HBase. Between Zohmg and Hadoop JSON is used
for serializing data.


\subsection{External}

The data export server is built with Python Paste---a middleware
framework---the data server dispatches to the requested application
depending on the URI it was fed.


\subsubsection{URLParser}

Middleware which dispatches to applications based on the URI. The URLParser
removes the first part of the URI and tries to find a matching application
to relay the request to. For instance, consider the URI
\texttt{/client/file.html}. The first part---\texttt{client}---will be
removed and the request \texttt{/file.html} will be sent to the application
named \texttt{client.py}.


\subsubsection{Clients}

Dispatches request to serve static files from the project's
\texttt{clients} directory. Custom clients---which interact with Zohmg---can
be built with HTML and JavaScript, see the examples.


\subsubsection{Data}

Fetches data from HBase via its Thrift interface. The data is served to the
user as JSONP\footnote{JSON with padding: JSON extension which adds name of
callback function as an argument to the function itself.}.


\subsubsection{Transformers}

Fetches data from HBase and transforms it with the requested transform
program from the project's transformers directory, output is
dumped as JSONP.



\chapter{Usage}

We have built a system---henceforth called Zohmg---which has the goal of being
a large scale aggregating data warehouse for time-series-based log-like
data sources.



\section{Deployment}

A manage script is used to create tables from a pre-defined schema.

\begin{verbatim}
   zohmg create project
\end{verbatim}

\noindent Edit the configuration files, which are text files in YAML and
Python format.

\begin{verbatim}
   zohmg setup
\end{verbatim}

\noindent The above command executes the configuration made and creates the
infrastructure (HBase tables) for the project.


\section{Import}

The user writes the map part of the MapReduce programs which imports
log-like data to the data store. The reduce step is taken care of by Zohmg,
which uses sum reducer that simply sums up values with the same keys.


\subsection{User mappers}

Import jobs use mappers written by the user, either in Python or Java.
Mappers written by the user import each line from the input file and set
the arguments to the map function, key and value, as line number and line
contents, respectively. The user can then perform computations on these
entities and finally yield the results as a dictionary. The resulting
dictionary will be interpreted by Zohmg and stored accordingly.

\begin{verbatim}
   zohmg import mappers/somemapper.py hdfs/path
\end{verbatim}


\section{Storing data}

Data is stored in Apache HBase. A natural consequence of this that the
data is stored sorted by rowkey allowing fast access to an arbitrary
(sequential) range.



\section{Export}

It is possible to query Zohmg about metadata it has on each of the imported
data sets.

Data is served to the user either over HTTP in JSON format or over Thrift.

\begin{verbatim}
   zohmg serve
\end{verbatim}

\noindent The above serves on default port 8086, with the \texttt{--port}
argument it is possible to change server port.

The data server have three methods of serving data: raw, transformed, or
static. The raw and transformed data are aggregates served from HBase,
transformed in the latter case, while the static method just serve static
files from the project's \texttt{clients} directory. Aggregates served from
HBase are returned to the user as JSONP.


\subsection{Data}

Based on the query---which the user submits---the corresponding data is
served from HBase as JSONP. The query is a simple definition of wanted row
range, dimensions, and values for these dimensions. The query is submitted
to the data server via HTTP, an example query could look like (the URL is
wrapped to fit the page) \\

\texttt{http://localhost:8086/data/?t0=20090101 \\
\&t1=20090420\&unit=pageviews\&d0=country\&d0v=SE} \\

The above query would return the pageviews for SE between 1 January 2009
and 4 April 2009.


\subsection{Transformers}

Fetches data from HBase and transforms it with the requested transform
program from the project's \texttt{transformers} directory, output is
dumped as JSONP.


\subsection{Clients}

It is possible to build static web pages out of HTML and JavaScript, which
can interact with the data server. These static pages can for instance
present some sort of query building mechanism---for instance with drop down
boxes---and then send this query to the data server and graph the resulting
data.

The data server serves static files from the project's \texttt{clients}
directory at the server URL appended with \texttt{client/filename}.



\chapter{Examples}

\section{Web log analysis}

Configure the dataset.yaml.

Write a custom mapper, which imports data from web logs.

Start data server and extract data from it.

Possibly write a transformer which performs some transformation on the
aggregated data.

Possibly write a client which will render a graph.



\chapter{Discussion}

\section{Related work}

HDW is system which goal is to create a high performance large scale data
warehouse based on MapReduce and Bigtable. \cite{hdw}



\section{Future work}

Says Wikipedia: "Organizations generally start off with relatively simple use of data warehousing. Over time, more sophisticated use of data warehousing evolves." Zohmg is that relatively simple data warehouse; the seed from which a more sophisticated data warehouse may evolve.

There is as of yet no support for specifying granularities. The only supported resolution of time is the day.

The single aggregation performed is summing of the measurements. Future work may include average, max and min, etc.

There is no metadata store. It would be very helpful for the application writer to know what the known values are for each dimension.

\subsection{Multiple data sets}

\subsection{Metadata}

\subsection{Updating data set files}

Update tables on changes in configuration---data set files.

\subsection{Importing without MapReduce}

Importing content without firing of a MapReduce job, e.g. via a Thrift
service.



\chapter{Conclusion}

Last.fm is looking at using Zohmg for multiple application areas, such as
radio stats and web page stats.


\pagebreak



\bibliographystyle{plain} \bibliography{references}



\end{document}
