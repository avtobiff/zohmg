hello!

let's get started with zohmg.

the outline of the working directory for your project looks like this:
config/
lib/
mappers/
scripts/


you will write the configuration file config.yaml that goes into
config, and you will write one or more mappers which go into the
mappers directory.

you will use the scripts in the scripts directory to make things
happen. the code that runs behind the scenes is in lib.


0)

Pre-requisites:
 - hadoop
 - hbase, with a running thrift server
 - dumbo


1) setting things up.

first, you'll want to define what dimensions your data has and what
projections of that data you are interested in.

open up config/config.yaml and enter something like this:

project_name:
 webbertricks

dimensions:
  - country
  - domain
  - useragent
  - usertype
  - referrer

units:
  - pageviews
  - visitors

projections:
    country:
      - country
    domain:
      - domain
    country-domain-useragent:
      - country
      - domain
      - useragent
    country-useragent:
      - country
      - useragent


This defines a few dimensions and a couple of units along with four
projections. The projections are a hint to what queries we will want
to ask the system later.

Your mapper will output measurements counted in these units, and each
measurement will be put along the axes of the dimensions.

Run 'python scripts/setup.py' and lean back as it creates an hbase
table for you project.



2) Importing.

Now we're ready to put some actual data into the data store!

We will want to write a mapper that maps each line of our log
files to a measurement along the dimension axes. Your mapper goes in
the mapper directory. It might look something like this:


def map(key, value):
    from lfm.data.parse import web

    try: log = web.Log(value)
    except ValueError: return
    ua = web.UserAgent()

    ts = log.timestamp.ymd()
    dimensions = {'country'   : log.country(),
                  'domain'    : log.domain,
                  'useragent' : ua.classify(log.agent),
                  'usertype'  : ("user", "anon")[log.userid == None]
                  }
    values = {'pageviews' : 1}

    yield ts, dimensions, values


The map function is really a generator; it can yield zero or more times
for every line of the log file.

The output of this mapper goes into a reducer that, for each
dimension-tuple, sums the values of each unit, does a magical dance
and eventually passes them along to the actual underlying data store.

We rely on dumbo to run our job.



3) Exporting.

