hello!

let's get started with zohmg.

the outline of the working directory for your project looks like this:

    config/
        dataset.yaml
        environment.py
    lib/
    mappers/
    scripts/


You will define some environment variables in environment.py and write the
configuration file dataset.yaml-- both goes into config--and you will
write one or more mappers which go into the mappers directory.

you will use the scripts in the scripts directory to make things
happen. the code that runs behind the scenes is in lib.



0) pre-requisities.

Install
- dumbo 0.21
- hadoop 0.19 (patched with 1722 and 5450 as per http://wiki.github.com/klbostee/dumbo/building-and-installing)
- hadoop streaming (it's a contrib!)
- hbase 0.19, with a running thrift server

# ALSO:
python's yaml library
python's simplejson library


1) creating a project.

Create the project from the command line by invoking, choose a
project name of your liking.

    zohmg create <project name>

If all goes well this creates a project directory in the your
current directory with the name "project name".



2) setting things up.

In the project directory there is a subdirectory called "config",
within there are two files: dataset.yaml and environment.py.

environment.py
--------------
Enter paths to Hadoop and HBase directories as well as define the
path to the required jar files.


dataset.yaml
-------------
you'll want to define what dimensions your data has and what
projections of that data you are interested in.

open up config/dataset.yaml and enter something like this:

dataset:
 webbertricks

dimensions:
  - country
  - domain
  - useragent
  - usertype
  - referrer

units:
  - pageviews
  - visitors

projections:
    country:
      - country
    domain:
      - domain
    country-domain-useragent:
      - country
      - domain
      - useragent
    country-useragent:
      - country
      - useragent


This defines a few dimensions and a couple of units along with four
projections. The projections are a hint to what queries we will want
to ask the system later.

Your mapper will output measurements counted in these units, and each
measurement will be put along the axes of the dimensions.

To create an HBase table for your project invoke the following and
lean back and enjoy the melodies.

    zohmg setup



3) Importing.

Now we're ready to put some actual data into the data store!

We will want to write a mapper that maps each line of our log
files to a measurement along the dimension axis. Your mapper goes in
the mapper directory. It might look something like this:


def map(key, value):
    from lfm.data.parse import web

    try: log = web.Log(value)
    except ValueError: return
    ua = web.UserAgent()

    ts = log.timestamp.ymd()
    dimensions = {'country'   : log.country(),
                  'domain'    : log.domain,
                  'useragent' : ua.classify(log.agent),
                  'usertype'  : ("user", "anon")[log.userid == None]
                  }
    values = {'pageviews' : 1}

    yield ts, dimensions, values


The map function is really a generator; it can yield zero or more times
for every line of the log file.

The output of this mapper goes into a reducer that, for each
dimension-tuple, sums the values of each unit, does a magical dance
and eventually passes them along to the actual underlying data store.

To run your import job invoke

    zohmg import mapper/your-mapper.py input/from/hdfs



4) Exporting.

Getting zohmg to serve data is as simple as running scripts/serve.py.

The pre-requisites are that you have hbase's thrift server running, and
- obviously - that there is some data in the store.

Run the data server with

    zohmg serve


try it out:
~zohmg$> python scripts/serve.py
Wed Apr  8 10:55:54 2009 - service ready.
serving data on http://127.0.0.1:8086
serving interface on http://127.0.0.1:8000

ok, good!

-- So, what's really is going on here? --


