
INTRODUCING: ZOHMG.
-------------------

Zohmg is a data store for aggregation of multi-dimensional time series data. It
is built on top of Hadoop, Dumbo and HBase. The core idea is to pre-compute
aggregates â€“ Zohmg is wasteful with storage in order to answer queries faster.

This README assumes a working installation of zohmg. Please see INSTALL for
installation instructions.

Zohmg is alpha software. Be gentle.


Website: http://github.com/zohmg/zohmg
IRC: #zohmg @ irc.freenode.net



RUN THE ZOHMG
-------------

Zohmg is installed as a command line script. Try it out:

	$> zohmg help



CREATE YOUR FIRST 'PROJECT'
---------------------------

    $> zohmg create weblogs

This command creates a project directory named "weblogs". Its contents are:

config       - environment and dataset configuration.
lib          - eggs or jars that will be automatically included in job jar.
mappers      - user defined MapReduce maps.
transformers - user defined data extraction transformations.



CONFIGURE YOUR 'PROJECT'
------------------------

The next step is to configure environment.py and dataset.yaml.


config/environment.py:

Define HADOOP_HOME and paths to all three jars (hadoop, hadoop-streaming,
hbase). You might need to run 'ant package' in $HADOOP_HOME to have all jars
built for you.


config/dataset.yaml:

Define your data set. This means setting up the dimensions and projections that will
be used. The dimensions lets Zohmg know what your data might look like while the
projections list hints at what queries you will ask the data store.

Example config/dataset.yaml:

    dataset: weblogs

    dimensions:
     - user
     - status

    projections:
     - user
     - status
     - user-status

    units:
     - pageviews


After you've edited environment.py and dataset.yaml:

    $> zohmg setup

This command creates an HBase table with the same name as your dataset.

Verify that the table was created:

    $> hbase shell
    hbase(main):001:0> list
    weblogs
    1 row(s) in 0.1337 seconds

    # ok!



RUN YOUR MAPPER
---------------

After the project is created and setup correctly it is time to import some
data. This is done in two steps: Write a map function and then run that
function over the data, which is stored on HDFS.

Firstly, writing the mapper. (Assuming Apache common log format, see
http://httpd.apache.org/docs/2.0/logs.html for more information.)

In the arguments to map there are key and value. The argument key is the
row number in the file being processed, the value argument is the actual
line data from the file.

In this example, the access time, user (HTTP auth) and the status for each
request are requested and passed along to the data store.

    def map(key,value):
        # parse log string, extract year, month and day
        import re
        mo = re.search(r"\[(\d{2})/(\w{3})/(\d{4})",value) # [20/Apr/2009
        year  = mo.group(3)
        month = mo.group(2)
        day   = mo.group(1)

        time  = year + month + day
        dimensions = {'user'   : value.split(" ")[2],
                      'status' : value.split(" ")[8]
                     }
        values = {'pageviews' : 1}

        yield time,dimensions,values


The output of the mapper goes into a reducer that, for each
dimension-tuple, sums the values of each unit and passes them along to the
actual underlying data stare. Make sure the values are of type int.

Secondly, run the mapper

	$> zohmg import mappers/mapper.py /data/weblogs/2009/05/11

The first argument to import is the path to the python file containing the map
function, the second is a path on HDFS.



SERVE DATA
----------

Assuming the mapper finished successfully there is now some data in
the HBase table.

Verify this by firing up the HBase shell:

       $> hbase shell
       hbase(main):001:0> scan 'weblogs'
       [..]

Lots of data scrolling by? Good! (Interrupt with CTRL-C at your leisure.)


Start the Zohmg server:

	$> zohmg serve

The Zohmg server listens on port 8086 at localhost by default. It uses
a Thrift interface for HBase queries, so make sure the thrift server is
on (i.e. 'hbase thrift start').



The API
-------

Zohmg's data server exposes the data store through an HTTP API. Every
request returns a JSON string.

The API is extremely simple: it supports a single type of GET request,
and there is no authentication.


There are four required parameters: t0, t1, unit and d0.

The parameters t0 and t1 define the time span. They are strings of the
format "yyyymmdd", i.e. "20090603".

The unit parameter defines the one unit for which you query, for
example 'pageviews'.

The parameter d0 defines the base dimension, for example 'country'.

Values for d0 can be defined by setting the parameter d0v to a
comma-separated string of values, for example "US,SE,DE". If d0v is
empty, all values of the base dimension will be returned.


A typical query string looks like this:

 http://localhost:8086/?t0=20090501&t1=20090531&unit=pageviews&d0=country&d0v=DE,SE,US

This example query would return pageview data for the time span between the
first and last of May broken down by the countries Sweden, Germany and
the United States.

The returned JSON is a list of dictionaries, like so:

[{"20090101": {"DE": 270, "US": 21, "SE": 5547}}, {"20090102": {"DE":
9020, "US": 109, "SE": 11497}}, {"20090103": {"DE": 10091, "US": 186,
"SE": 8863}}]



Plotting the data
-----------------

There is a graphing tool bundled with Zohmg. It is served statically
by the Zohmg server at http://localhost:8086/static/index.html

The graphing tool uses Javascript to query the data server, and plots
graphs with the help of Google Charts.

