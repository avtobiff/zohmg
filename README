
INTRODUCING: ZOHMG.
-------------------

Zohmg is a data store for aggregation of multi-dimensional time series data. It
is built on top of Hadoop, Dumbo and HBase. The core idea is to pre-compute
aggregates â€“ Zohmg is wasteful with storage in order to answer queries faster.

This README assumes a working installation of zohmg. Please see INSTALL for
installation instructions.

Zohmg is alpha software. Be gentle.


CONTACT
-------

IRC: #zohmg at freenode
Web: http://zohmg.com/ (soon)
Code: http://github.com/zohmg/zohmg/tree/master
Mailing list: http://groups.google.com/group/zohmg-user


RUN THE ZOHMG
-------------

Zohmg is installed as a command line script. Try it out:

	$> zohmg help



CREATE YOUR FIRST 'PROJECT'
---------------------------

    $> zohmg create weblogs

This command creates a project directory named "weblogs".

Its contents are:
config       - environment and dataset configuration.
lib          - eggs or jars that will be automatically included in job jar.
mappers      - user defined MapReduce maps.


CONFIGURE YOUR 'PROJECT'
------------------------

The next step is to configure environment.py and dataset.yaml.


config/environment.py:

Define HADOOP_HOME and set the paths for all three jars (hadoop, hadoop-streaming,
hbase). You might need to run 'ant package' in $HADOOP_HOME to have
the streaming jar built for you.


config/dataset.yaml:

Define your data set. This means setting up the dimensions and
projections that will be used. The dimensions lets Zohmg know what
your data looks like while the projections list hints at what queries
you will ask the data store.


Example config/dataset.yaml:

    dataset:
      weblogs

    dimensions:
     - user
     - status

    projections:
     - user
     - status
     - user-status

    units:
     - pageviews


After you've edited environment.py and dataset.yaml:

    $> zohmg setup

This command creates an HBase table with the same name as your dataset.

Verify that the table was created:

    $> hbase shell
    hbase(main):001:0> list
    weblogs
    1 row(s) in 0.1337 seconds

    # ok!



WRITE A  MAPPER
----------------

After the project is created and setup correctly it is time to import some
data. This is done in two steps: Write a map function and then run that
function over the data, which is stored on HDFS.

Firstly, writing the mapper. (Assuming Apache common log format, see
http://httpd.apache.org/docs/2.0/logs.html for more information.)

In the arguments to map there are key and value. The argument key is the
row number in the file being processed, the value argument is the actual
line data from the file.

In this example, the access time, user (HTTP auth) and the status for each
request are requested and passed along to the data store.

    def map(key,value):
        # parse log string, extract year, month and day
        import re
        mo = re.search(r"\[(\d{2})/(\w{3})/(\d{4})",value) # [20/Apr/2009
        year  = mo.group(3)
        month = mo.group(2)
        day   = mo.group(1)

        time  = year + month + day
        dimensions = {'user'   : value.split(" ")[2],
                      'status' : value.split(" ")[8]
                     }
        values = {'pageviews' : 1}

        yield time,dimensions,values


The output of the mapper goes into a reducer that, for each
dimension-tuple, sums the values of each unit and passes them along to the
actual underlying data stare. Make sure the values are of type int.



RUN THE MAPPER
--------------

Secondly, run the mapper:

    $> zohmg import mappers/mapper.py /data/weblogs/2009/06/05

The first argument to import is the path to the python file containing the map
function, the second is a path on HDFS.


You can enable lzo compression by giving --lzo as the third argument:

    $> zohmg import mappers/mapper.py /data/weblogs/2009/06/05 --lzo


Any argument after the first two (and except for --lzo) will be passed
on to Dumbo.  If you wish to pass jobconf options, for example,
you would do it like so:

    $> zohmg import mappers/mapper.py /data/weblogs/2009/06/05 -hadoopconf someoption=yeah

The Dumbo wiki has more information on what options other than
hadoopconf are supported: http://wiki.github.com/klbostee/dumbo/running-programs



SERVE DATA
----------

Assuming the mapper finished successfully there is now some data in
the HBase table.

Verify this by firing up the HBase shell:

       $> hbase shell
       hbase(main):001:0> scan 'weblogs'
       [..]

Lots of data scrolling by? Good! (Interrupt with CTRL-C at your leisure.)


Start the Zohmg server:

	$> zohmg server

The Zohmg server listens on port 8086 at localhost by default. It uses
a Thrift interface for HBase queries, so make sure the thrift server is
on (i.e. 'hbase thrift start').



THE API
-------

Zohmg's data server exposes the data store through an HTTP API. Every
successful request returns a JSON-formatted string.

The JSON looks something like this:

[{"20090601": {"DE": 270, "US": 21, "SE": 5547}}, {"20090602": {"DE":
9020, "US": 109, "SE": 11497}}, {"20090603": {"DE": 10091, "US": 186,
"SE": 8863}}]


The API is extremely simple: it supports a single type of GET request
and there is no authentication.

There are four required parameters: t0, t1, unit and d0.

The parameters t0 and t1 define the time span. They are strings of the
format "yyyymmdd", i.e. "t0=20090601&t1=20090630".

The unit parameter defines the one unit for which you query, for
example 'unit=pageviews'.

The parameter d0 defines the base dimension, for example 'd0=country'.

Values for d0 can be defined by setting the parameter d0v to a
comma-separated string of values, for example "d0v=US,SE,DE". If d0v is
empty, all values of the base dimension will be returned.


A typical query string looks like this:

 http://localhost:8086/?t0=20090601&t1=20090630&unit=pageviews&d0=country&d0v=DE,SE,US

This example query would return pageview data for the time span between the
first and last of June broken down by the countries Sweden, Germany and
the United States.


The API supports JSONP via the jsonp parameter. By setting this
parameter, the returned JSON is wrapped in a function call.



PLOTTING THE DATA
-----------------

There is an example client bundled with Zohmg. It is served by the Zohmg server
at http://localhost:8086/graph/ and is quite useful for exploring your dataset.
You may also want to peek at the javascript code to gain some inspiration and
insight into how the beast works.

The graphing tool uses Javascript to query the data server, and plots
graphs with the help of Google Charts.



KNOWN ISSUES
------------

Zohmg is alpha software and is still learning how to behave properly in mixed
company. You will spot more than a few minor quirks while using it and might
perhaps even run into the odd show-stopper. If you do, please let us know!

One of the most troublesome issues is that there is a definite limit on the
number of data points that can be stored for any projection. Currently we're
seeing slow response times for projections with more than a few hundred
thousand combinations. Hopefully this will alleviated with HBase 0.20.

Zohmg currently only supports mappers written in Python. Eventually, you will
also be able to write mappers in Java.
