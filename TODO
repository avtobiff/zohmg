priority zero:
DONE> make sure all dependent eggs (paste, json, yaml, dumbo) are installed.
DONE> ImportError: No module named setuptools
DONE> ImportError: No module named hbase.ttypes

priority uno:
+ rename. (zonotoop)
+ make sure it's easy to get zohmg up and running.
DONE>  - shell script to install dependencies
DONE> + server
DONE>  - transformers, plugin to data server
+ config
  - put address and ports for servers and software in config (hbase REST et al)
+ some update step for post-setup alterations to dataset.yaml
+ add support for data sets.
+ create a source dist and a binary dist.
 - source dist, would distribute source and depend on thrift etc to build.
 - binary dist, ready to use eggs.
DONE> hbase and thrift eggs system-wideness necessary in order to keep zohmg.utils happy.
DONE> add the eggs to site-packages/easy-install.pth (using setuptools?!)
DONE> bundle generated code and build eggs?
* Build HBase thrift interface from actual installed version of thrift
* Ask user for path to Hbase.thrift and build hbase-*.egg from it.
+ verify dimensions and units outputted by mapper against those in config.
DONE> project creation
DONE> define exact dependencies, including patches, etc.
DONE> in config, rename 'project_name' to 'dataset'.
DONE> creation of dist tarball
DONE> in config, rename 'project_name' to 'dataset'.
DONE> sanity check of environment; issue warning if HADOOP_HOME missing, jars not files, etc.
DONE> sanity check configuration.
DONE> darling: remove grat. prints.


priority due:
+ (maybe never) support auto-installation on non-debian system.
+ (maybe never) support for installing in user-home
+ add description of what zohmg is and its long term goal.
+ sanity check for files shipped off to dumbo/hadoop.
  - check for existance.
  - especially for environment stuff (*_HOME and CLASSPATH).
+ better way of importing usermapper.
  - adding -pyfile to dumbo.core?
    * dumbo always does a local run before hadoop, can fail if file is not
      in PYTHONPATH (-file does not put files in PYTHONPATH, naturally).
    * set PYTHONPATH locally is the requirement.
  - "more careful" way of importing usermapper (think Mapper constructor).
+ document how user contributed code is shipped to dumbo/hadoop.
SKIP> add lfm.data.parse egg.

examples:
 js-or-what:
  + serve html&js - maybe with paste.fileapp? => http://pythonpaste.org/modules/fileapp.html
  + cache JSON and CHARTS.
  + connect metadata to select-dropdowns.
