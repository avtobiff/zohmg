priority uno:
+ rename.
+ make sure it's easy to get zohmg up and running.
  - shell script to install dependencies
DONE>  - project creation
+ server
  - transformers, plugin to data server
+ config
  - put address and ports for servers and software in config (hbase REST et al)
+ some update step for post-setup alterations to dataset.yaml
+ add support for data sets.
+ define exact dependencies, including patches, etc.
+ hbase and thrift eggs system-wideness necessary in order to keep zohmg.utils happy.
  - add the eggs to site-packages/easy-install.pth (using setuptools?!)
  - bundle generated code and build eggs?
    * Build HBase thrift interface from actual installed version of thrift
    * Ask user for path to Hbase.thrift and build hbase-*.egg from it.
+ in config, rename 'project_name' to 'dataset'.
DONE> + creation of dist tarball
DONE> in config, rename 'project_name' to 'dataset'.
DONE> + sanity check of environment; issue warning if HADOOP_HOME missing, jars not files, etc.
DONE> sanity check configuration.
+ verify dimensions and units outputted by mapper against those in config.

priority due:
+ serve html&js - maybe with paste.fileapp? => http://pythonpaste.org/modules/fileapp.html
+ cache JSON and CHARTS.
+ connect metadata to select-dropdowns.
+ sanity check for files shipped off to dumbo/hadoop.
  - check for existance.
  - especially for environment stuff (*_HOME and CLASSPATH).
+ better way of importing usermapper.
  - adding -pyfile to dumbo.core?
    * dumbo always does a local run before hadoop, can fail if file is not
      in PYTHONPATH (-file does not put files in PYTHONPATH, naturally).
    * set PYTHONPATH locally is the requirement.
  - "more careful" way of importing usermapper (think Mapper constructor).
+ add lfm.data.parse egg.
  - document how user contributed code is shipped to dumbo/hadoop.
